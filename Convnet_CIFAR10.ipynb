{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convnet CIFAR10.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2QIK3XQiocB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sat Apr 25 19:12:42 2020\n",
        "\n",
        "@author: Jana\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU\n",
        "device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
        "def dist_matplt(tensor,tup_shape,title):\n",
        "    plt.title(title)\n",
        "    flatten = 1\n",
        "    for i in tup_shape:\n",
        "        flatten = flatten * i\n",
        "    x = tensor.view(-1,flatten).detach().numpy()\n",
        "    plt.hist(x[0],bins=50)\n",
        "    plt.show()\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print('\\t'.join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = '{:' + str(num_digits) + 'd}'\n",
        "        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n",
        "def binarize(quant_mode = 'det'):\n",
        "    class sign(torch.autograd.Function):\n",
        "        @staticmethod\n",
        "        def forward(ctx, input):\n",
        "            ctx.save_for_backward(input)\n",
        "            if quant_mode=='det':\n",
        "                input = input.sign()\n",
        "            else:\n",
        "                input = input.add_(1).div_(2).add_(torch.rand(input.size()).add(-0.5)).clamp_(0,1).round().mul_(2).add_(-1)\n",
        "            return input\n",
        "        @staticmethod\n",
        "        def backward(ctx, grad_output):\n",
        "            input, = ctx.saved_tensors\n",
        "            grad_input = grad_output.clone()\n",
        "            grad_input[input.ge(1)] = 0\n",
        "            grad_input[input.le(-1)] = 0\n",
        "            return grad_input\n",
        "    return sign().apply\n",
        "\n",
        "class weight_quantize_fn(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(weight_quantize_fn, self).__init__()\n",
        "    self.Binarize = binarize()\n",
        "\n",
        "  def forward(self, x):\n",
        "      #E = torch.mean(torch.abs(x)).detach()\n",
        "      weight_q = self.Binarize(x)\n",
        "      return weight_q\n",
        "def keep_elements_dict(vals, kwargs):\n",
        "    t = list(kwargs.keys())\n",
        "    for k in t:\n",
        "        if k not in vals:\n",
        "            del kwargs[k]\n",
        "class BinConv2d(nn.Conv2d):\n",
        "    def __init__(self,in_channels,out_channels,kernel_size,full_precision = False,**kwargs):\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.full_precision = full_precision\n",
        "        self.binarize_input = kwargs.get('binarize_input',True)\n",
        "        keep_elements_dict(['stride', 'padding', 'kernel_size','padding_mode','dilation','groups'],kwargs)\n",
        "        self.stride = kwargs.get('stride',1)\n",
        "        self.padding = kwargs.get('padding',0)\n",
        "        self.padding_mode = kwargs.get('padding_mode','zeros')\n",
        "        self.dilation = kwargs.get('dilation',1)\n",
        "        self.groups = kwargs.get('groups',1)\n",
        "        self.bias = False\n",
        "        super(BinConv2d,self).__init__(self.in_channels,self.out_channels,self.kernel_size,**kwargs)\n",
        "        #sns.distplot(self.weight.detach().numpy())\n",
        "        self.reset_parameters()\n",
        "        self.Binarize = weight_quantize_fn()\n",
        "    def forward(self,input):\n",
        "        if not self.full_precision:\n",
        "            self.Modweight = self.Binarize(self.weight)\n",
        "            if self.binarize_input:\n",
        "              input = self.Binarize(input)\n",
        "        else:\n",
        "            self.Modweight = self.weight\n",
        "        self.out = nn.functional.conv2d(input,self.Modweight,None)\n",
        "        return self.out\n",
        "    def reset_parameters(self):\n",
        "        self.kaming_uniform()\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            init.uniform_(self.bias, -bound, bound)\n",
        "    #Weight Initialization\n",
        "    def kaming_uniform(self):\n",
        "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "    def uniform(self):\n",
        "        init.uniform_(self.weight, a=0.0, b=1.0)\n",
        "    def xavier_normal(self):\n",
        "        nn.init.xavier_normal_(self.weight)\n",
        "    def kaming_normal(self):\n",
        "         nn.init.kaiming_normal_(self.weight, mode='fan_out')\n",
        "            \n",
        "class BinLinear(nn.Linear):\n",
        "    def __init__(self,in_channels,out_channels,full_precision = False,**kwargs):\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.full_precision = full_precision\n",
        "        self.bias = False\n",
        "        super(BinLinear,self).__init__(self.in_channels,self.out_channels)\n",
        "        #sns.distplot(self.weight.detach().numpy())\n",
        "        self.reset_parameters()\n",
        "        self.Binarize = weight_quantize_fn()\n",
        "    def forward(self,input):\n",
        "        if not self.full_precision:\n",
        "            self.Modweight = self.Binarize(self.weight)\n",
        "            input = self.Binarize(input)\n",
        "        else:\n",
        "            self.Modweight = self.weight\n",
        "        self.out = nn.functional.linear(input,self.Modweight,None)\n",
        "        return self.out\n",
        "    def reset_parameters(self):\n",
        "        self.kaming_uniform()\n",
        "        if self.bias is not None:\n",
        "            fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n",
        "            bound = 1 / math.sqrt(fan_in)\n",
        "            init.uniform_(self.bias, -bound, bound)\n",
        "    #Weight Initialization\n",
        "    def kaming_uniform(self):\n",
        "        init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
        "    def uniform(self):\n",
        "        init.uniform_(self.weight, a=0.0, b=1.0)\n",
        "    def xavier_normal(self):\n",
        "        nn.init.xavier_normal_(self.weight)\n",
        "    def kaming_normal(self):\n",
        "         nn.init.kaiming_normal_(self.weight, mode='fan_out')\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.infl_ratio=1\n",
        "        self.fc1 = BinLinear(784, 2048*self.infl_ratio,True)\n",
        "        self.htanh1 = nn.Hardtanh()\n",
        "        self.bn1 = nn.BatchNorm1d(2048*self.infl_ratio)\n",
        "        self.fc2 = BinLinear(2048*self.infl_ratio, 2048*self.infl_ratio,True)\n",
        "        self.htanh2 = nn.Hardtanh()\n",
        "        self.bn2 = nn.BatchNorm1d(2048*self.infl_ratio)\n",
        "        self.fc3 = BinLinear(2048*self.infl_ratio, 2048*self.infl_ratio,True)\n",
        "        self.htanh3 = nn.Hardtanh()\n",
        "        self.bn3 = nn.BatchNorm1d(2048*self.infl_ratio)\n",
        "        self.fc4 = BinLinear(2048*self.infl_ratio, 10,True)\n",
        "        self.logsoftmax=nn.LogSoftmax()\n",
        "        self.drop=nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = self.fc1(x)\n",
        "        #x = self.bn1(x)\n",
        "        x = self.htanh1(x)\n",
        "        x = self.fc2(x)\n",
        "        #x = self.bn2(x)\n",
        "        x = self.htanh2(x)\n",
        "        x = self.fc3(x)\n",
        "        #x = self.bn3(x)\n",
        "        x = self.htanh3(x)\n",
        "        x = self.fc4(x)\n",
        "        out = self.logsoftmax(x)\n",
        "        #dist_matplt(out,out.shape,\"softmax\")\n",
        "        return out\n",
        "\n",
        "class VGG_Cifar10(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(VGG_Cifar10, self).__init__()\n",
        "        self.infl_ratio=1\n",
        "        self.features = nn.Sequential(\n",
        "            BinConv2d(3, 128*self.infl_ratio, kernel_size=3,full_precision = True),\n",
        "            nn.BatchNorm2d(128*self.infl_ratio),\n",
        "            #nn.Hardtanh(inplace=True),\n",
        "            nn.ReLU(inplace = True),\n",
        "\n",
        "            BinConv2d(128*self.infl_ratio, 128*self.infl_ratio, kernel_size=3,full_precision = True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(128*self.infl_ratio),\n",
        "            #nn.Hardtanh(inplace=True),\n",
        "            nn.ReLU(inplace = True),\n",
        "\n",
        "            BinConv2d(128*self.infl_ratio, 256*self.infl_ratio, kernel_size=3,full_precision = True),\n",
        "            nn.BatchNorm2d(256*self.infl_ratio),\n",
        "            #nn.Hardtanh(inplace=True),\n",
        "            nn.ReLU(inplace = True),\n",
        "\n",
        "            BinConv2d(256*self.infl_ratio, 256*self.infl_ratio, kernel_size=3,full_precision = True),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(256*self.infl_ratio),\n",
        "            #nn.Hardtanh(inplace=True),\n",
        "            nn.ReLU(inplace = True),\n",
        "\n",
        "            BinConv2d(256*self.infl_ratio, 512*self.infl_ratio, kernel_size=3, full_precision = True),\n",
        "            nn.BatchNorm2d(512*self.infl_ratio),\n",
        "            #nn.Hardtanh(inplace=True),\n",
        "            nn.ReLU(inplace = True),\n",
        "\n",
        "            BinConv2d(512*self.infl_ratio, 512, kernel_size=3, full_precision = True),\n",
        "            #nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(512),\n",
        "            #nn.Hardtanh(inplace=True)\n",
        "            nn.ReLU(inplace = True)\n",
        "\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            BinLinear(512 * 1 * 1, 1024,True),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            #nn.Hardtanh(inplace=True),\n",
        "            nn.ReLU(inplace = True),\n",
        "            #nn.Dropout(0.5),\n",
        "            BinLinear(1024, 1024,True),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            #nn.Hardtanh(inplace=True),\n",
        "            nn.ReLU(inplace = True),\n",
        "            #nn.Dropout(0.5),\n",
        "            BinLinear(1024, num_classes,True), #Full Presicision\n",
        "            nn.LogSoftmax()\n",
        "        )\n",
        "\n",
        "        self.regime = {\n",
        "            0: {'optimizer': 'Adam', 'betas': (0.9, 0.999),'lr': 5e-3},\n",
        "            40: {'lr': 1e-3},\n",
        "            80: {'lr': 5e-4},\n",
        "            100: {'lr': 1e-4},\n",
        "            120: {'lr': 5e-5},\n",
        "            140: {'lr': 1e-5}\n",
        "        }\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(-1, 512 * 1 * 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "#Accuracy\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res    \n",
        "from collections import Counter\n",
        "\n",
        "def no_of_params(weights):\n",
        "    weights = weights.detach().numpy()\n",
        "    weights = weights.flatten()\n",
        "    weights = weights.tolist()\n",
        "    counter = Counter(weights)\n",
        "    print(counter)\n",
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    # switch to train mode\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "    progress = ProgressMeter(\n",
        "        len(train_loader),\n",
        "        [losses, top1, top5],\n",
        "        prefix=\"Epoch: [{}]\".format(epoch))\n",
        "    model.train()\n",
        "    add_param = []\n",
        "    for i, (images, target) in enumerate(train_loader):\n",
        "        \n",
        "        # compute output\n",
        "        output = model(images.type(dtype))\n",
        "        loss = criterion(output, target.to(device))\n",
        "\n",
        "        # measure accuracy and record loss\n",
        "        acc1, acc5 = accuracy(output, target.to(device), topk=(1, 5))\n",
        "        losses.update(loss.detach().item(), images.size(0))\n",
        "        top1.update(acc1[0], images.size(0))\n",
        "        top5.update(acc5[0], images.size(0))\n",
        "        # compute gradient and do SGD step\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if i % 100 == 0:\n",
        "            progress.display(i)\n",
        "    print(\"epoch : {}, accuray: {},Loss : {} \".format(epoch,top1.avg,losses.avg))\n",
        "    import numpy as np\n",
        "    add_param.append(epoch)\n",
        "    add_param.append(losses.avg)\n",
        "    add_param.append((top1.avg).tolist())\n",
        "    add_param.append((top5.avg).tolist())\n",
        "    trainframe.loc[epoch] = add_param\n",
        "\n",
        "def validate(val_loader, model, criterion, epoch):\n",
        "    losses = AverageMeter('Loss', ':.4e')\n",
        "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "    progress = ProgressMeter(\n",
        "        len(val_loader),\n",
        "        [losses, top1, top5],\n",
        "        prefix='Test: ')\n",
        "    # switch to evaluate mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        add_param = []\n",
        "        add_param.append(epoch)\n",
        "        for i, (images, target) in enumerate(val_loader):\n",
        "            # compute output\n",
        "            output = model(images.type(dtype))\n",
        "            loss = criterion(output, target.to(device))\n",
        "\n",
        "            # measure accuracy and record loss\n",
        "            acc1, acc5 = accuracy(output, target.to(device), topk=(1, 5))\n",
        "            losses.update(loss.detach().item(), images.size(0))\n",
        "            top1.update(acc1[0], images.size(0))\n",
        "            top5.update(acc5[0], images.size(0))\n",
        "            if i % 100 == 0:\n",
        "              progress.display(i)\n",
        "        add_param.append(losses.avg)\n",
        "        add_param.append((top1.avg).tolist())\n",
        "        add_param.append((top5.avg).tolist())\n",
        "        testframe.loc[epoch] = add_param\n",
        "        # TODO: this should also be done with the ProgressMeter\n",
        "        print(\"Test epoch : {}, accuray: {},Loss : {} \".format(epoch,top1.avg,losses.avg))\n",
        "        return top1.avg\n",
        "def adjust_optimizer(optimizer, epoch, config): \n",
        "    __optimizers = {\n",
        "    'SGD': torch.optim.SGD,\n",
        "    'ASGD': torch.optim.ASGD,\n",
        "    'Adam': torch.optim.Adam,\n",
        "    'Adamax': torch.optim.Adamax,\n",
        "    'Adagrad': torch.optim.Adagrad,\n",
        "    'Adadelta': torch.optim.Adadelta,\n",
        "    'Rprop': torch.optim.Rprop,\n",
        "    'RMSprop': torch.optim.RMSprop\n",
        "    }\n",
        "    \"\"\"Reconfigures the optimizer according to epoch and config dict\"\"\"\n",
        "    def modify_optimizer(optimizer, setting):\n",
        "        if 'optimizer' in setting:\n",
        "            optimizer = __optimizers[setting['optimizer']](\n",
        "                optimizer.param_groups)\n",
        "        for param_group in optimizer.param_groups:\n",
        "            for key in param_group.keys():\n",
        "                if key in setting:\n",
        "                    param_group[key] = setting[key]\n",
        "        return optimizer\n",
        "    for e in range(epoch + 1):  # run over all epochs - sticky setting\n",
        "        if e in config:\n",
        "            optimizer = modify_optimizer(optimizer, config[e])\n",
        "\n",
        "    return optimizer    \n",
        "import os\n",
        "def save_checkpoint(state, is_best, path='.', filename='checkpoint.pth.tar', save_all=False):\n",
        "    filename = os.path.join(path, filename)\n",
        "    torch.save(state, filename)        \n",
        "if __name__ == '__main__':\n",
        "    #dtype = torch.float\n",
        "    #device = torch.device(\"cpu\")\n",
        "    dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU\n",
        "    device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
        "    torch.backends.cudnn.benchmark = True #Uncomment this to run on GPU\n",
        "    best_prec1 = 0\n",
        "    import pandas as pd\n",
        "    #trainframe = pd.DataFrame(columns =['epoch','loss','accuracy1','accuracy5','fc1P1','fc1N1','fc2P1','fc2N1','fc3P1','fc3N1','fc4P1','fc4N1'])\n",
        "    trainframe = pd.DataFrame(columns =['epoch','loss','accuracy1','accuracy5'])\n",
        "    testframe = pd.DataFrame(columns =['epoch','loss','accuracy1','accuracy5'])\n",
        "    import torch\n",
        "    import torchvision\n",
        "    from torchvision import transforms\n",
        "    print('==> Preparing data..')\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR10(\n",
        "        root='C:\\\\Users\\\\ajana\\\\data', train=True, download=True, transform=transform_train)\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        trainset, batch_size=128, shuffle=True, num_workers=1)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(\n",
        "        root='C:\\\\Users\\\\ajana\\\\data', train=False, download=True, transform=transform_test)\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        testset, batch_size=1000, shuffle=False, num_workers=1)\n",
        "    start_epoch = 1\n",
        "    epochs = 121\n",
        "    import torch.optim as optim\n",
        "    model = VGG_Cifar10()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    criterion.type(dtype)\n",
        "    model.type(dtype)\n",
        "    lr = 0.01\n",
        "    momentum = 0.9\n",
        "    weight_decay = 1e-4\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "    #regime = getattr(model, 'regime', {0: {'optimizer': optimizer,\n",
        "    #                                       'lr': lr,\n",
        "    #                                       'momentum': momentum,\n",
        "    #                                       'weight_decay': weight_decay}})\n",
        "    optimizer = optim.Adam(model.parameters(),lr = lr)\n",
        "\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "        #optimizer = adjust_optimizer(optimizer, epoch, regime)\n",
        "        train(train_loader, model, criterion, optimizer, epoch)\n",
        "        acc1 = validate(test_loader, model, criterion, epoch)\n",
        "        is_best = acc1 > best_prec1\n",
        "        best_prec1 = max(acc1, best_prec1)\n",
        "\n",
        "        save_checkpoint({\n",
        "            'epoch': epoch,\n",
        "            'state_dict': model.state_dict(),\n",
        "            'best_prec1': best_prec1,\n",
        "        }, is_best, path=\"drive/My Drive/Model Results/\",filename='checkpoint_cifar10)_0.01.pth.tar')\n",
        "        trainframe.to_excel(\"drive/My Drive/Model Results/Cifar10_train_0.01.xlsx\")\n",
        "        testframe.to_excel(\"drive/My Drive/Model Results/Cifar10_test_0.01.xlsx\") \n",
        "        \n",
        "print(trainframe,testframe)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}