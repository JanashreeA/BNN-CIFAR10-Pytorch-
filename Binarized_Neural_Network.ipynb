{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Binarized Neural Network.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzzZC-2jEAqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets,transforms\n",
        "import time\n",
        "import math\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from collections import OrderedDict\n",
        "dtype = (torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.backends.cudnn.benchmark = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EK9fy3YGboJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    is_debug = True\n",
        "    import pandas as pd\n",
        "    trainframe = pd.DataFrame(columns =['epoch','loss','accuracy1','accuracy5'])\n",
        "    testframe = pd.DataFrame(columns =['epoch','loss','accuracy1','accuracy5'])\n",
        "    #User Definable\n",
        "    dataset = \"CIFAR10\"\n",
        "    start_epoch = 1\n",
        "    epochs = 6\n",
        "    lr = 0.01\n",
        "    filename = \"Sample_binresnet\"\n",
        "    model =AlexNet(10)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(),lr = lr)\n",
        "    \n",
        "    is_sch_epoch = False\n",
        "    is_sch_acc = False\n",
        "    is_sch_valloss = False\n",
        "    is_reg = False\n",
        "    is_drive = False\n",
        "    \n",
        "    dataloaders, dataset_sizes = getDataset(dataset)\n",
        " \n",
        "    criterion.type(dtype)\n",
        "    model.type(dtype)\n",
        "    \n",
        "    model = train_model(model, criterion, optimizer,num_epochs=epochs,is_drive=is_drive)\n",
        "    print(trainframe,testframe)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0af53k2lbsmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getDataset(dataset,train_batch_size = 128, test_batch_size = 1000,train_data_shuffle = True, test_data_shuffle = False):\n",
        "    if dataset == \"MNIST\":\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "          torchvision.datasets.MNIST('C:\\\\Users\\\\ajana\\\\data', train=True, download=True,\n",
        "                          transform=transforms.Compose([\n",
        "                              transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.1307,), (0.3081,))\n",
        "                          ])),\n",
        "          batch_size=train_batch_size, shuffle=True)\n",
        "        test_loader = torch.utils.data.DataLoader(\n",
        "          torchvision.datasets.MNIST('C:\\\\Users\\\\ajana\\\\data', train=False, transform=transforms.Compose([\n",
        "                              transforms.ToTensor(),\n",
        "                              transforms.Normalize((0.1307,), (0.3081,))\n",
        "                          ])),\n",
        "          batch_size=test_batch_size, shuffle=True)\n",
        "        dataloaders = {\"train\":train_loader,\"val\":test_loader}\n",
        "        dataset_sizes = {\"train\":60000,\"val\":10000}\n",
        "    if dataset == \"CIFAR10\":\n",
        "        transform_train = transforms.Compose([\n",
        "              transforms.RandomCrop(32, padding=4),\n",
        "              transforms.RandomHorizontalFlip(),\n",
        "              transforms.ToTensor(),\n",
        "              transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "          ])\n",
        "\n",
        "        transform_test = transforms.Compose([\n",
        "              transforms.ToTensor(),\n",
        "              transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "          ])\n",
        "\n",
        "        trainset = torchvision.datasets.CIFAR10(\n",
        "            root='C:\\\\Users\\\\ajana\\\\data', train=True, download=True, transform=transform_train)\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            trainset, batch_size=train_batch_size, shuffle=train_data_shuffle, num_workers=1)\n",
        "\n",
        "        testset = torchvision.datasets.CIFAR10(\n",
        "            root='C:\\\\Users\\\\ajana\\\\data', train=False, download=True, transform=transform_test)\n",
        "        test_loader = torch.utils.data.DataLoader(\n",
        "            testset, batch_size=test_batch_size, shuffle=test_data_shuffle, num_workers=1)\n",
        "        dataloaders = {\"train\":train_loader,\"val\":test_loader}\n",
        "        dataset_sizes = {\"train\":len(trainset),\"val\":len(testset)}\n",
        "    if dataset == \"CIFAR100\":\n",
        "        transform_train = transforms.Compose([\n",
        "                transforms.RandomCrop(32, padding=4),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "            ])\n",
        "        transform_test = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "            ])\n",
        "        trainset = torchvision.datasets.CIFAR100(root = 'C:\\\\Users\\\\ajana\\\\data', train=True, transform=transform_train, download=True)\n",
        "        train_loader =  torch.utils.data.DataLoader(trainset, batch_size=train_batch_size, shuffle=train_data_shuffle, num_workers=2)\n",
        "        testset = datasets.CIFAR100(root = 'C:\\\\Users\\\\ajana\\\\data', train=False, transform=transform_test, download=True)\n",
        "        test_loader =  torch.utils.data.DataLoader(testset, batch_size=test_batch_size, shuffle=test_data_shuffle, num_workers=2)\n",
        "        dataloaders = {\"train\":train_loader,\"val\":test_loader}\n",
        "        dataset_sizes = {\"train\":len(trainset),\"val\":len(testset)}\n",
        "    if is_debug:\n",
        "        print(\"Loaded Train set of size : {} into loader with num of batches {}\".format(dataset_sizes['train'],len(dataloaders['train'])))\n",
        "        print(\"Loaded Test set of size : {} into loader with num of batches {}\".format(dataset_sizes['val'],len(dataloaders['val'])))\n",
        "    return dataloaders,dataset_sizes\n",
        "def no_of_params(weights):\n",
        "  weights = weights.detach().numpy()\n",
        "  weights = weights.flatten()\n",
        "  weights = weights.tolist()\n",
        "  counter = Counter(weights)\n",
        "  print(counter)\n",
        "def binarize(quant_mode = 'det',ste = \"ste_backward\",**kwargs):\n",
        "    class sign(torch.autograd.Function):\n",
        "        @staticmethod\n",
        "        def forward(ctx, input):\n",
        "            ctx.save_for_backward(input)\n",
        "            if quant_mode=='det':\n",
        "                input = input.sign()\n",
        "            else:\n",
        "                input = input.add_(1).div_(2).add_(torch.rand(input.size()).add(-0.5)).clamp_(0,1).round().mul_(2).add_(-1)\n",
        "            #with torch.no_grad():\n",
        "                #no_of_params(input)\n",
        "            return input\n",
        "        @staticmethod\n",
        "        def backward(ctx, grad_output):\n",
        "            input, = ctx.saved_tensors\n",
        "            if ste == \"ste_backward\":\n",
        "                grad_input = ste_backward(input, grad_output)\n",
        "                return grad_input\n",
        "            if ste == \"tanh_backward\":\n",
        "                grad_input = tanh_backward(input, grad_output)\n",
        "                return grad_input\n",
        "            if ste == \"htanh_backward\":\n",
        "                grad_input = htanh_backward(input, grad_output)\n",
        "                return grad_input\n",
        "            if ste == \"swish_backward\":\n",
        "                beta = kwargs.get('beta')\n",
        "                grad_input,beta = swish_backward(input, grad_output, beta)\n",
        "                return grad_input,beta\n",
        "    return sign().apply\n",
        "\n",
        "class weight_quantize_fn(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(weight_quantize_fn, self).__init__()\n",
        "    self.Binarize = binarize()\n",
        "\n",
        "  def forward(self, x):\n",
        "      #E = torch.mean(torch.abs(x)).detach()\n",
        "      weight_q = self.Binarize(x)\n",
        "      return weight_q\n",
        "\n",
        "def ste_backward(input, grad_output):\n",
        "  grad_input = grad_output.clone()\n",
        "  grad_input[input.ge(1)] = 0\n",
        "  grad_input[input.le(-1)] = 0\n",
        "  return grad_input, None\n",
        "\n",
        "def htanh_backward(input, grad_output):\n",
        "  grad_input = grad_output.clone()\n",
        "  grad_input[input.ge(1/3)] = 0\n",
        "  grad_input[input.le(-1/3)] = 0\n",
        "  return 3.0 * grad_input, None\n",
        "\n",
        "def tanh_backward(input, grad_output):\n",
        "  input2 = 2 * input \n",
        "  z = 1 - input2.tanh()**2\n",
        "  grad_input = grad_output.clone() * z\n",
        "  return grad_input,None\n",
        "\n",
        "def soft_swish(x, beta):\n",
        "  loss = beta * (2 - beta * x * torch.tanh(beta * x / 2)) \\\n",
        "          / (1 + (torch.exp(beta * x) + torch.exp(-beta * x)) / 2)\n",
        "  return loss\n",
        "\n",
        "def swish_backward(input, grad_output, beta):\n",
        "  grad_input = grad_output.clone()\n",
        "  z = soft_swish(input, beta)\n",
        "  grad_input = grad_output.clone() * z\n",
        "  return grad_input, beta\n",
        "\n",
        "def keep_elements_dict(vals, kwargs):\n",
        "    t = list(kwargs.keys())\n",
        "    for k in t:\n",
        "        if k not in vals:\n",
        "            del kwargs[k]       \n",
        "class BinConv2d(nn.Conv2d):\n",
        "    def __init__(self,in_channels,out_channels,kernel_size,full_precision = False,**kwargs):\n",
        "        self.scale = kwargs.get('scale', False)\n",
        "        self.compute_scale = kwargs.get('compute_scale', 'topk')\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.full_precision = full_precision\n",
        "        self.binarize_input = kwargs.get('binarize_input',True)\n",
        "        keep_elements_dict(['stride', 'padding', 'kernel_size','padding_mode','dilation','groups'],kwargs)\n",
        "        self.stride = kwargs.get('stride',1)\n",
        "        self.padding = kwargs.get('padding',0)\n",
        "        self.padding_mode = kwargs.get('padding_mode','zeros')\n",
        "        self.dilation = kwargs.get('dilation',1)\n",
        "        self.groups = kwargs.get('groups',1)\n",
        "        self.bias = False\n",
        "        super(BinConv2d,self).__init__(self.in_channels,self.out_channels,self.kernel_size,**kwargs)\n",
        "        self.Binarize = weight_quantize_fn()\n",
        "    def forward(self,input):\n",
        "        if not self.full_precision:\n",
        "            if self.scale and not hasattr(self, 'alpha'):\n",
        "                if self.compute_scale == 'mean':\n",
        "                    with torch.no_grad():\n",
        "                        tk = torch.mean(self.weight.view(self.weight.shape[0], -1), dim=1)\n",
        "                elif self.compute_scale == 'topk':\n",
        "                    with torch.no_grad():\n",
        "                        p = 0.5\n",
        "                        k = torch.ceil((1-p) * torch.prod(torch.tensor(self.weight.shape[1:]), dtype=torch.float))\n",
        "                        tk = torch.topk(torch.abs(self.weight.view(self.weight.shape[0], -1)), int(k), dim=1)[0][:, -1]\n",
        "                elif self.compute_scale != \"\":\n",
        "                    with torch.no_grad():\n",
        "                        val = float(self.compute_scale)\n",
        "                        tk = val * torch.ones(self.weight.shape[0], requires_grad=True)\n",
        "                else:\n",
        "                    raise NotImplementedError(\"compute_scale not implemented\")\n",
        "                tk = tk.to(self.weight.device)\n",
        "\n",
        "                self.alpha = nn.Parameter(tk[:, None], requires_grad=True)\n",
        "            self.Modweight = self.Binarize(self.weight)\n",
        "            if self.binarize_input:\n",
        "                input = self.Binarize(input)\n",
        "        else:\n",
        "            self.Modweight = self.weight\n",
        "        self.out = nn.functional.conv2d(input,self.Modweight,None,padding=self.padding,stride=self.stride)\n",
        "        return self.out\n",
        "class BinLinear(nn.Linear):\n",
        "    def __init__(self,in_channels,out_channels,full_precision = False,**kwargs):\n",
        "        self.scale = kwargs.get('scale', False)\n",
        "        self.compute_scale = kwargs.get('compute_scale', 'topk')\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.full_precision = full_precision\n",
        "        self.bias = False\n",
        "        super(BinLinear,self).__init__(self.in_channels,self.out_channels)\n",
        "        self.Binarize = weight_quantize_fn()\n",
        "    def forward(self,input):\n",
        "        if not self.full_precision:\n",
        "            if self.scale and not hasattr(self, 'alpha'):\n",
        "                if self.compute_scale == 'mean':\n",
        "                    with torch.no_grad():\n",
        "                        tk = torch.mean(self.weight.view(self.weight.shape[0], -1), dim=1)\n",
        "                elif self.compute_scale == 'topk':\n",
        "                    with torch.no_grad():\n",
        "                        p = 0.5\n",
        "                        k = torch.ceil((1-p) * torch.prod(torch.tensor(self.weight.shape[1:]), dtype=torch.float))\n",
        "                        tk = torch.topk(torch.abs(self.weight.view(self.weight.shape[0], -1)), int(k), dim=1)[0][:, -1]\n",
        "                elif self.compute_scale != \"\":\n",
        "                    with torch.no_grad():\n",
        "                        val = float(self.compute_scale)\n",
        "                        tk = val * torch.ones(self.weight.shape[0], requires_grad=True)\n",
        "                else:\n",
        "                    raise NotImplementedError(\"compute_scale not implemented\")\n",
        "                tk = tk.to(self.weight.device)\n",
        "\n",
        "                self.alpha = nn.Parameter(tk[:, None], requires_grad=True)\n",
        "            self.Modweight = self.Binarize(self.weight)\n",
        "            input = self.Binarize(input)\n",
        "        else:\n",
        "            self.Modweight = self.weight\n",
        "        self.out = nn.functional.linear(input,self.Modweight,None)\n",
        "        return self.out\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import torch.nn.functional as F\n",
        "from collections import OrderedDict\n",
        "class BaseModel(nn.Module):\n",
        "    \"\"\"An abstract class representing a model architecture.\n",
        "    Any model definition should subclass `BaseModel`.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.size = 0\n",
        "        self.kaming_uniform()\n",
        "        self.batch_norm_init()\n",
        "\n",
        "    @property\n",
        "    def num_params(self):\n",
        "        return sum(param.numel() for param in self.parameters())\n",
        "\n",
        "    def num_trainable_params(self):\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)  # Trainable parameters\n",
        "    def size_of_params(self):             \n",
        "        for name,param in self.named_parameters():\n",
        "            if (((\"Conv2d\" in name and \"Bin\" in name) or (\"Linear\" in name and \"Bin\" in name)) and \"classifier\" not in name) and \"bias\" not in name:\n",
        "                print(name)\n",
        "                self.size = self.size + param.numel()\n",
        "        print(\"-\"*50)\n",
        "        print(\"Cumulative size of all binary layers is : {} MB\".format(self.size/8000000))\n",
        "        print(\"-\"*50)\n",
        "        for name,param in self.named_parameters():\n",
        "            if ((\"Conv2d\" in name and \"Bin\" not in name) or \"classifier\" in name or (\"Linear\" in name and \"Bin\" not in name)) and \"bias\" not in name and \"BN\" not in name:\n",
        "                print(name)\n",
        "                self.size = self.size + param.numel()*32\n",
        "        print(\"Cumulative size of all layers without bn is : {} MB\".format(self.size/8000000))\n",
        "        print(\"-\"*50)\n",
        "        for name,param in self.named_parameters():\n",
        "            if \"BN\"  in name and \"bias\" not in name:\n",
        "                print(name)\n",
        "                self.size = self.size + param.numel()*32\n",
        "        print(\"Cumulative size of all layers with bn is : {} MB\".format(self.size/8000000))\n",
        "        print(\"-\"*50) \n",
        "        for name,param in self.named_parameters():\n",
        "            if \"Act\"  in name and \"bias\" not in name:\n",
        "                print(name)\n",
        "                self.size = self.size + param.numel()\n",
        "        print(\"Cumulative size of all layers  : {} MB\".format(self.size/8000000))\n",
        "        print(\"-\"*50)  \n",
        "        self.size = self.size / 8000000\n",
        "    \n",
        "    def xavier_init(self):\n",
        "      # default xavier init\n",
        "      print(\"Xavier Init\")\n",
        "      for m in self.modules():\n",
        "          if isinstance(m, (BinConv2d, BinLinear)):\n",
        "              nn.init.xavier_uniform(m.weight)\n",
        "        \n",
        "    def he_et_al_init(self):\n",
        "      # he initialization\n",
        "      print(\"He et al Init\")\n",
        "      for m in self.modules():\n",
        "          if isinstance(m, (BinConv2d, BinLinear)):\n",
        "              nn.init.kaiming_normal_(m.weight, mode='fan_in')\n",
        "    \n",
        "    def kaming_uniform(self):\n",
        "      # he initialization\n",
        "      print(\"Kaming Uniform Init\")\n",
        "      for m in self.modules():\n",
        "        if isinstance(m, (BinConv2d, BinLinear)):\n",
        "            nn.init.kaiming_uniform_(m.weight, a=math.sqrt(5))\n",
        "\n",
        "    def orthogonal_init(self):\n",
        "      print(\"Orthogonal Init\")\n",
        "      for m in self.modules():\n",
        "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "            nn.init.orthogonal(m.weight)\n",
        "\n",
        "    def batch_norm_init(self):\n",
        "      print(\"BatchNorm Init made weight as 1 and bias as 0\")\n",
        "      for m in self.modules():\n",
        "        if isinstance(m, nn.BatchNorm2d):\n",
        "            nn.init.constant(m.weight, 1)\n",
        "            nn.init.constant(m.bias, 0)\n",
        "\n",
        "    def binary_parameters(self):\n",
        "        for name, layer in self.named_parameters():\n",
        "            if \"Bin\" in name and \"Conv2d\" in name and \"bias\" not in name:\n",
        "                print(name)\n",
        "                yield layer\n",
        "\n",
        "    def non_binary_parameters(self):\n",
        "        for name, layer in self.named_parameters():\n",
        "            if ((\"BN\" in name)  or (\"Conv2d in name\" and \"Bin\" not in name)) and (\"bias\" not in name):\n",
        "                yield layer\n",
        "    def forward(self, x):\n",
        "        raise NotImplementedError\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.infl_ratio=1\n",
        "        self.Linear_1 = BinLinear(784, 2048*self.infl_ratio,True)\n",
        "        self.Act1 = nn.Hardtanh()\n",
        "        self.BN1 = nn.BatchNorm1d(2048*self.infl_ratio)\n",
        "        self.BinLinear_2 = BinLinear(2048*self.infl_ratio, 2048*self.infl_ratio)\n",
        "        self.Act2 = nn.Hardtanh()\n",
        "        self.BN2 = nn.BatchNorm1d(2048*self.infl_ratio)\n",
        "        self.BinLinear_3 = BinLinear(2048*self.infl_ratio, 2048*self.infl_ratio)\n",
        "        self.Act3 = nn.Hardtanh()\n",
        "        self.BN3 = nn.BatchNorm1d(2048*self.infl_ratio)\n",
        "        self.Linear_4 = BinLinear(2048*self.infl_ratio, 10,True)\n",
        "        self.BN4 = nn.BatchNorm1d(10)\n",
        "        self.logsoftmax=nn.LogSoftmax()\n",
        "        self.drop=nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = self.Linear_1(x)\n",
        "        x = self.BN1(x)\n",
        "        x = self.Act1(x)\n",
        "        x = self.BinLinear_2(x)\n",
        "        x = self.BN2(x)\n",
        "        x = self.Act2(x)\n",
        "        x = self.BinLinear_3(x)\n",
        "        x = self.BN3(x)\n",
        "        x = self.Act3(x)\n",
        "        x = self.Linear_4(x)\n",
        "        x = self.BN4(x)\n",
        "        out = self.logsoftmax(x)\n",
        "        return out\n",
        "\n",
        "cfg_bin = {\n",
        "    'VGG11': ['F', 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': ['F', 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': ['F', 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': ['F', 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "class VGG_BNN_ReLU(BaseModel):\n",
        "    def __init__(self, vgg_name, nclass, img_width=32):\n",
        "        super(VGG_BNN_ReLU, self).__init__()\n",
        "        self.img_width = img_width\n",
        "        self.nclass = nclass\n",
        "        self.features = self._make_layers(cfg_bin[vgg_name])\n",
        "        self.classifier = self._make_classifier()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out # return None, to make it compatible with VGG_noise\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        iMax = 1\n",
        "        iBn = 1\n",
        "        iConv = 1\n",
        "        iAct = 1\n",
        "        in_channels = 3\n",
        "        width = self.img_width\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers.append((\"Max_{}\".format(iMax),nn.MaxPool2d(kernel_size=2, stride=2)))\n",
        "                iMax = iMax + 1\n",
        "                width = width // 2\n",
        "            elif x == 'F':\n",
        "                x = 64\n",
        "                layers.append((\"Conv2d_{}\".format(iConv),nn.Conv2d(in_channels, x, kernel_size=3, padding=1)))\n",
        "                layers.append((\"BN_{}\".format(iBn),nn.BatchNorm2d(x)))\n",
        "                layers.append((\"Act_{}\".format(iAct),nn.ReLU(inplace=True)))\n",
        "                iConv = iConv + 1\n",
        "                iBn = iBn + 1\n",
        "                iAct = iAct + 1\n",
        "                in_channels = 64\n",
        "            else:\n",
        "                layers.append((\"BinConv2d_{}\".format(iConv),BinConv2d(in_channels, x, kernel_size=3, padding=1)))\n",
        "                layers.append((\"BN_{}\".format(iBn),nn.BatchNorm2d(x)))\n",
        "                layers.append((\"Act_{}\".format(iAct),nn.ReLU(inplace=True)))\n",
        "                iConv = iConv + 1\n",
        "                iBn = iBn + 1\n",
        "                iAct = iAct + 1\n",
        "                in_channels = x\n",
        "        self.iBn = iBn\n",
        "        self.iConv = iConv\n",
        "        self.iAct = iAct\n",
        "        layers.append((\"Dropout_1\",nn.Dropout(0.5)))\n",
        "        layer_ord = OrderedDict(layers)\n",
        "        return nn.Sequential(layer_ord)\n",
        "    def _make_classifier(self):\n",
        "        layers = []\n",
        "        layers.append((\"Linear_{}\".format(self.iConv),nn.Linear(512, self.nclass)))\n",
        "        layers.append((\"BN_{}\".format(self.iBn),nn.BatchNorm1d(self.nclass)))\n",
        "        layers.append((\"Softmax\",nn.LogSoftmax()))\n",
        "        layer_ord = OrderedDict(layers)\n",
        "        return nn.Sequential(layer_ord)\n",
        "    \n",
        "class VGG_BNN_PReLU(BaseModel):\n",
        "    def __init__(self, vgg_name, nclass, img_width=32):\n",
        "        super(VGG_BNN_PReLU, self).__init__()\n",
        "        self.img_width = img_width\n",
        "        self.nclass = nclass\n",
        "        self.features = self._make_layers(cfg_bin[vgg_name])\n",
        "        self.classifier = self._make_classifier()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out # return None, to make it compatible with VGG_noise\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        iMax = 1\n",
        "        iBn = 1\n",
        "        iConv = 1\n",
        "        iAct = 1\n",
        "        in_channels = 3\n",
        "        width = self.img_width\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers.append((\"Max_{}\".format(iMax),nn.MaxPool2d(kernel_size=2, stride=2)))\n",
        "                iMax = iMax + 1\n",
        "                width = width // 2\n",
        "            elif x == 'F':\n",
        "                x = 64\n",
        "                layers.append((\"Conv2d_{}\".format(iConv),nn.Conv2d(in_channels, x, kernel_size=3, padding=1)))\n",
        "                layers.append((\"BN_{}\".format(iBn),nn.BatchNorm2d(x)))\n",
        "                layers.append((\"Act_{}\".format(iAct),nn.PReLU()))\n",
        "                iConv = iConv + 1\n",
        "                iBn = iBn + 1\n",
        "                iAct = iAct + 1\n",
        "                in_channels = 64\n",
        "            else:\n",
        "                layers.append((\"BinConv2d_{}\".format(iConv),BinConv2d(in_channels, x, kernel_size=3, padding=1)))\n",
        "                layers.append((\"BN_{}\".format(iBn),nn.BatchNorm2d(x)))\n",
        "                layers.append((\"Act_{}\".format(iAct),nn.PReLU()))\n",
        "                iConv = iConv + 1\n",
        "                iBn = iBn + 1\n",
        "                iAct = iAct + 1\n",
        "                in_channels = x\n",
        "        self.iBn = iBn\n",
        "        self.iConv = iConv\n",
        "        self.iAct = iAct\n",
        "        layers.append((\"Dropout_1\",nn.Dropout(0.5)))\n",
        "        layer_ord = OrderedDict(layers)\n",
        "        return nn.Sequential(layer_ord)\n",
        "    def _make_classifier(self):\n",
        "        layers = []\n",
        "        layers.append((\"Linear_{}\".format(self.iConv),nn.Linear(512, self.nclass)))\n",
        "        layers.append((\"BN_{}\".format(self.iBn),nn.BatchNorm1d(self.nclass)))\n",
        "        layers.append((\"Softmax\",nn.LogSoftmax()))\n",
        "        layer_ord = OrderedDict(layers)\n",
        "        return nn.Sequential(layer_ord)\n",
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "class VGG_ReLU(BaseModel):\n",
        "    def __init__(self, vgg_name, nclass, img_width=32):\n",
        "        super(VGG_ReLU, self).__init__()\n",
        "        self.img_width = img_width\n",
        "        self.nclass = nclass\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier =  self._make_classifier()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out # return None, to make it compatible with VGG_noise\n",
        "    \n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        iMax = 1\n",
        "        iBn = 1\n",
        "        iConv = 1\n",
        "        iAct = 1\n",
        "        in_channels = 3\n",
        "        width = self.img_width\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers.append((\"Max_{}\".format(iMax),nn.MaxPool2d(kernel_size=2, stride=2)))\n",
        "                iMax = iMax + 1\n",
        "                width = width // 2\n",
        "            else:\n",
        "                layers.append((\"Conv2d_{}\".format(iConv),nn.Conv2d(in_channels, x, kernel_size=3, padding=1)))\n",
        "                layers.append((\"BN_{}\".format(iBn),nn.BatchNorm2d(x)))\n",
        "                layers.append((\"Act_{}\".format(iAct),nn.ReLU(inplace=True)))\n",
        "                iConv = iConv + 1\n",
        "                iBn = iBn + 1\n",
        "                iAct = iAct + 1\n",
        "                in_channels = x\n",
        "        self.iBn = iBn\n",
        "        self.iConv = iConv\n",
        "        self.iAct = iAct\n",
        "        layers.append((\"Dropout_1\",nn.Dropout(0.5)))\n",
        "        layer_ord = OrderedDict(layers)\n",
        "        return nn.Sequential(layer_ord)\n",
        "    def _make_classifier(self):\n",
        "        layers = []\n",
        "        layers.append((\"Linear_{}\".format(self.iConv),nn.Linear(512, self.nclass)))\n",
        "        layers.append((\"BN_{}\".format(self.iBn),nn.BatchNorm1d(self.nclass)))\n",
        "        layers.append((\"Softmax\",nn.LogSoftmax()))\n",
        "        layer_ord = OrderedDict(layers)\n",
        "        return nn.Sequential(layer_ord)\n",
        "#CNN ALexnet (While initializing give num_classes )\n",
        "class AlexNet(BaseModel):\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features =  nn.ModuleDict({\n",
        "            \"Conv2d_1\":nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            #\"BN_1\":nn.BatchNorm2d(64),\n",
        "            \"Act_1\":nn.ReLU(inplace=True),\n",
        "            \"Max_1\":nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            \"Conv2d_2\":nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
        "            #\"BN_2\":nn.BatchNorm2d(192),\n",
        "            \"Act_2\":nn.ReLU(inplace=True),\n",
        "            \"Max_2\":nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            \"Conv2d_3\":nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
        "            #\"BN_3\":nn.BatchNorm2d(384),\n",
        "            \"Act_3\":nn.ReLU(inplace=True),\n",
        "            \"Conv2d_4\":nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
        "            #\"BN_4\":nn.BatchNorm2d(64),\n",
        "            \"Act_4\":nn.ReLU(inplace=True),\n",
        "            \"Conv2d_5\":nn.Conv2d(256, 256, kernel_size=3, padding=2),\n",
        "            #\"BN_5\":nn.BatchNorm2d(64),\n",
        "            \"Act_5\":nn.ReLU(inplace=True),\n",
        "            \"Max_5\":nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        })\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "\n",
        "        self.fullyconnected = nn.ModuleDict({\n",
        "            \"Pool\":nn.AdaptiveAvgPool2d((6, 6)),\n",
        "            \"drop_6\":nn.Dropout(),\n",
        "            \"Linear_6\":nn.Linear(256 * 6 * 6, 4096),\n",
        "            #\"BN_6\":nn.BatchNorm1d(4096),\n",
        "            \"Act_6\":nn.ReLU(inplace=True),\n",
        "            \"drop_7\":nn.Dropout(),\n",
        "            \"Linear_7\":nn.Linear(4096, 4096),\n",
        "            #\"BN_7\":nn.BatchNorm1d(4096),\n",
        "            \"Act_7\":nn.ReLU(inplace=True),\n",
        "            \"Linear_8\":nn.Linear(4096, num_classes),\n",
        "            #\"BN_8\":nn.BatchNorm1d(num_classes),\n",
        "            #\"Softmax\":nn.LogSoftmax()\n",
        "        })\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features['Conv2d_1'](x)\n",
        "        x = self.features['Act_1'](x)\n",
        "        x = self.features['Max_1'](x)\n",
        "        x = self.features['Conv2d_2'](x)\n",
        "        x = self.features['Act_2'](x)\n",
        "        x = self.features['Max_2'](x)\n",
        "        x = self.features['Conv2d_3'](x)\n",
        "        x = self.features['Act_3'](x)\n",
        "        x = self.features['Conv2d_4'](x)\n",
        "        x = self.features['Act_4'](x)\n",
        "        x = self.features['Conv2d_5'](x)\n",
        "        x = self.features['Act_5'](x)\n",
        "        x = self.features['Max_5'](x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(-1, 256 * 6 * 6)\n",
        "        x = self.fullyconnected['Linear_6'](x)\n",
        "        x = self.fullyconnected['Act_6'](x)\n",
        "        x = self.fullyconnected['Linear_7'](x)\n",
        "        x = self.fullyconnected['Act_7'](x)\n",
        "        x = self.fullyconnected['Linear_8'](x)\n",
        "        return x\n",
        "#BNN ALexnet (While initializing give num_classes )\n",
        "#BNN ALexnet (While initializing give num_classes )\n",
        "class BinAlexNet(BaseModel):\n",
        "    def __init__(self, num_classes=1000):\n",
        "        super(BinAlexNet, self).__init__()\n",
        "        self.features =  nn.ModuleDict({\n",
        "            \"Conv2d_1\":nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
        "            \"BN_1\":nn.BatchNorm2d(64),\n",
        "            \"Act_1\":nn.ReLU(inplace=True),\n",
        "            \"Max_1\":nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            \"BinConv2d_2\":BinConv2d(64, 192, kernel_size=5, padding=2),\n",
        "            \"BN_2\":nn.BatchNorm2d(192),\n",
        "            \"Act_2\":nn.ReLU(inplace=True),\n",
        "            \"Max_2\":nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "            \"BinConv2d_3\":BinConv2d(192, 384, kernel_size=3, padding=1),\n",
        "            \"BN_3\":nn.BatchNorm2d(384),\n",
        "            \"Act_3\":nn.ReLU(inplace=True),\n",
        "            \"BinConv2d_4\":BinConv2d(384, 256, kernel_size=3, padding=1),\n",
        "            \"BN_4\":nn.BatchNorm2d(256),\n",
        "            \"Act_4\":nn.ReLU(inplace=True),\n",
        "            \"BinConv2d_5\":BinConv2d(256, 256, kernel_size=3, padding=2),\n",
        "            \"BN_5\":nn.BatchNorm2d(256),\n",
        "            \"Act_5\":nn.ReLU(inplace=True),\n",
        "            \"Max_5\":nn.MaxPool2d(kernel_size=3, stride=2,)\n",
        "            \n",
        "        })\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "\n",
        "        self.fullyconnected = nn.ModuleDict({\n",
        "            #\"drop_2:\"nn.Dropout(),\n",
        "            \"BinLinear_6\":BinLinear(256 * 6 * 6, 4096),\n",
        "            \"BN_6\":nn.BatchNorm1d(4096),\n",
        "            \"Act_6\":nn.ReLU(inplace=True),\n",
        "            #\"drop\":nn.Dropout(),\n",
        "            \"BinLinear_7\":BinLinear(4096, 4096),\n",
        "            \"BN_7\":nn.BatchNorm1d(4096),\n",
        "            \"Act_7\":nn.ReLU(inplace=True),\n",
        "            \"Linear_8\":nn.Linear(4096, num_classes),\n",
        "            \"BN_8\":nn.BatchNorm1d(num_classes),\n",
        "            \"Softmax\":nn.LogSoftmax()\n",
        "        })\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features['Conv2d_1'](x)\n",
        "        x = self.features['BN_1'](x)\n",
        "        x = self.features['Act_1'](x)\n",
        "        x = self.features['Max_1'](x)\n",
        "        x = self.features['BinConv2d_2'](x)\n",
        "        x = self.features['BN_2'](x)\n",
        "        x = self.features['Act_2'](x)\n",
        "        x = self.features['Max_2'](x)\n",
        "        x = self.features['BinConv2d_3'](x)\n",
        "        x = self.features['BN_3'](x)\n",
        "        x = self.features['Act_3'](x)\n",
        "        x = self.features['BinConv2d_4'](x)\n",
        "        x = self.features['BN_4'](x)\n",
        "        x = self.features['Act_4'](x)\n",
        "        x = self.features['BinConv2d_5'](x)\n",
        "        x = self.features['BN_5'](x)\n",
        "        x = self.features['Act_5'](x)\n",
        "        x = self.features['Max_5'](x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(-1, 256 * 6 * 6)\n",
        "        x = self.fullyconnected['BinLinear_6'](x)\n",
        "        x = self.fullyconnected['BN_6'](x)\n",
        "        x = self.fullyconnected['Act_6'](x)\n",
        "        x = self.fullyconnected['BinLinear_7'](x)\n",
        "        x = self.fullyconnected['BN_7'](x)\n",
        "        x = self.fullyconnected['Act_7'](x)\n",
        "        x = self.fullyconnected['Linear_8'](x)\n",
        "        x = self.fullyconnected['BN_8'](x)\n",
        "        x = self.fullyconnected['Softmax'](x)\n",
        "        return x\n",
        "\n",
        "#CNN net\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.Conv2d_1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.BN1 = nn.BatchNorm2d(planes)\n",
        "        self.Conv2d_2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.BN2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            layers = []\n",
        "            layers.append((\"Conv2d_3\",nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)))\n",
        "            layers.append((\"BN_3\",nn.BatchNorm2d(self.expansion*planes)))\n",
        "            layer_ord = OrderedDict(layers)\n",
        "            self.shortcut = nn.Sequential(layer_ord)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.BN1(self.Conv2d_1(x)))\n",
        "        out = self.BN2(self.Conv2d_2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.Conv2d_1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.BN1 = nn.BatchNorm2d(planes)\n",
        "        self.Conv2d_2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.BN2 = nn.BatchNorm2d(planes)\n",
        "        self.Conv2d_3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.BN3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            layers = []\n",
        "            layers.append((\"Conv2d_4\",nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)))\n",
        "            layers.append((\"BN_4\",nn.BatchNorm2d(self.expansion*planes)))\n",
        "            layer_ord = OrderedDict(layers)\n",
        "            self.shortcut = nn.Sequential(layer_ord)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.BN1(self.Conv2d_1(x)))\n",
        "        out = F.relu(self.BN2(self.Conv2d_2(out)))\n",
        "        out = self.BN3(self.Conv2d_3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class ResNet(BaseModel):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.Conv2d_1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.BN1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.Linear_2 = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.BN1(self.Conv2d_1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.Linear_2(out)\n",
        "        return out\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2,2,2,2])\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3,4,6,3])\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3,4,6,3])\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3,4,23,3])\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3,8,36,3])\n",
        "\n",
        "def test():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1,3,32,32))\n",
        "    print(y.size())\n",
        "#BNN net\n",
        "class BinBasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BinBasicBlock, self).__init__()\n",
        "        self.BinConv2d_1 = BinConv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.BN1 = nn.BatchNorm2d(planes)\n",
        "        self.BinConv2d_2 = BinConv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.BN2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            layers = []\n",
        "            layers.append((\"BinConv2d_3\",BinConv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)))\n",
        "            layers.append((\"BN_3\",nn.BatchNorm2d(self.expansion*planes)))\n",
        "            layer_ord = OrderedDict(layers)\n",
        "            self.shortcut = nn.Sequential(layer_ord)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.BN1(self.BinConv2d_1(x)))\n",
        "        out = self.BN2(self.BinConv2d_2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class BinBottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BinBottleneck, self).__init__()\n",
        "        self.BinConv2d_1 = BinConv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.BN1 = nn.BatchNorm2d(planes)\n",
        "        self.BinConv2d_2 = BinConv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.BN2 = nn.BatchNorm2d(planes)\n",
        "        self.BinConv2d_3 = BinConv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.BN3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            layers = []\n",
        "            layers.append((\"BinConv2d_4\",BinConv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)))\n",
        "            layers.append((\"BN_4\",nn.BatchNorm2d(self.expansion*planes)))\n",
        "            layer_ord = OrderedDict(layers)\n",
        "            self.shortcut = nn.Sequential(layer_ord)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.BN1(self.BinConv2d_1(x)))\n",
        "        out = F.relu(self.BN2(self.BinConv2d_2(out)))\n",
        "        out = self.BN3(self.BinConv2d_3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "class BinResNet(BaseModel):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(BinResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.Conv2d_1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.BN1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.Linear_2 = nn.Linear(512*block.expansion, num_classes)\n",
        "        self.BN2 = nn.BatchNorm1d(num_classes)\n",
        "        self.softmax = nn.LogSoftmax()\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.BN1(self.Conv2d_1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.Linear_2(out)\n",
        "        out = self.BN2(out)\n",
        "        out = self.softmax(out)\n",
        "        return out\n",
        "\n",
        "def BinResNet18():\n",
        "    return BinResNet(BinBasicBlock, [2,2,2,2])\n",
        "\n",
        "def BinResNet34():\n",
        "    return BinResNet(BinBasicBlock, [3,4,6,3])\n",
        "\n",
        "def BinResNet50():\n",
        "    return BinResNet(BinBottleneck, [3,4,6,3])\n",
        "\n",
        "def BinResNet101():\n",
        "    return BinResNet(BinBottleneck, [3,4,23,3])\n",
        "\n",
        "def BinResNet152():\n",
        "    return BinResNet(BinBottleneck, [3,8,36,3])\n",
        "\n",
        "def Bintest():\n",
        "    net = BinResNet18()\n",
        "    y = net(torch.randn(1,3,32,32))\n",
        "    print(y.size())\n",
        "\n",
        "def filt_alpha_weight(model):\n",
        "    alpha = []\n",
        "    weights = []\n",
        "    layers = {}\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            if ('alpha' in name):\n",
        "                n_split = name.split('alpha')\n",
        "                nn = n_split[0]\n",
        "                if (nn not in layers.keys()):\n",
        "                    layers[nn] = []\n",
        "                alpha.append(param)\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            if ('weight' in name) and \"BN\" not in name:\n",
        "                n_split = name.split('weight')\n",
        "                nn = n_split[0]\n",
        "                if (nn in list(layers.keys())):\n",
        "                    weights.append(param)\n",
        "\n",
        "    return alpha, weights\n",
        "import math\n",
        "def get_reg(model,reg_type,epoch,lr,eta = 1):\n",
        "    with torch.enable_grad():\n",
        "        l = eta * lr * math.log(epoch)\n",
        "        alpha, weights = filt_alpha_weight(model)\n",
        "        reg = 0\n",
        "        if reg_type == 'l1':\n",
        "            for a, w in zip(alpha, weights):\n",
        "                reg += torch.sum(torch.abs(torch.abs(w) - a))\n",
        "        elif reg_type == 'l2':\n",
        "            for a, w in zip(alpha, weights):\n",
        "                reg += torch.sum(torch.abs(torch.mul(w, w) - a))\n",
        "        elif reg_type == 'l2_2':\n",
        "            for a, w in zip(alpha, weights):\n",
        "                reg += torch.sum(torch.mul(w,w) - a)\n",
        "        elif reg_type == 'tmr1':\n",
        "            for a, w in zip(alpha, weights):\n",
        "                reg += torch.abs(torch.sum(torch.mul(w,w) - a))\n",
        "        elif reg_type == 'tmr2':\n",
        "            for a, w in zip(alpha, weights):\n",
        "                reg += torch.sum(torch.mul(w,w) - a)**2\n",
        "        else:\n",
        "            reg = 0\n",
        "    return l*reg\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "class EarlyStopping:\n",
        "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
        "    def __init__(self, patience=7, verbose=False, delta=0,filename = 'checkpoint.pt'):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patience (int): How long to wait after last time validation loss improved.\n",
        "                            Default: 7\n",
        "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
        "                            Default: False\n",
        "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
        "                            Default: 0\n",
        "        \"\"\"\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        '''Saves model when validation loss decrease.'''\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), filename)\n",
        "        self.val_loss_min = val_loss\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "  \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "  with torch.no_grad():\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "    res = []\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
        "        res.append(correct_k.mul_(100.0 / batch_size))\n",
        "    return res    \n",
        "\n",
        "class AverageMeter(object):\n",
        "  \"\"\"Computes and stores the average and current value\"\"\"\n",
        "  def __init__(self, name, fmt=':f'):\n",
        "      self.name = name\n",
        "      self.fmt = fmt\n",
        "      self.reset()\n",
        "\n",
        "  def reset(self):\n",
        "      self.val = 0\n",
        "      self.avg = 0\n",
        "      self.sum = 0\n",
        "      self.count = 0\n",
        "\n",
        "  def update(self, val, n=1):\n",
        "      self.val = val\n",
        "      self.sum += val * n\n",
        "      self.count += n\n",
        "      self.avg = self.sum / self.count\n",
        "\n",
        "  def __str__(self):\n",
        "      fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "      return fmtstr.format(**self.__dict__)\n",
        "        \n",
        "class ProgressMeter(object):\n",
        "  def __init__(self, num_batches, meters, prefix=\"\"):\n",
        "      self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "      self.meters = meters\n",
        "      self.prefix = prefix\n",
        "\n",
        "  def display(self, batch):\n",
        "      entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "      entries += [str(meter) for meter in self.meters]\n",
        "      print('\\t'.join(entries))\n",
        "\n",
        "  def _get_batch_fmtstr(self, num_batches):\n",
        "      num_digits = len(str(num_batches // 1))\n",
        "      fmt = '{:' + str(num_digits) + 'd}'\n",
        "      return '[' + fmt + '/' + fmt.format(num_batches) + ']'  \n",
        "def train_model(model, criterion, optimizer,num_epochs=25,**kwargs):    \n",
        "    since = time.time()\n",
        "    best_acc = 0.0\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    fig = plt.figure(figsize=(12, 8))\n",
        "    fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
        "    is_early_stopping = kwargs.get('is_early_stopping',True)\n",
        "    is_sch_epoch = kwargs.get('is_sch_epoch',False)\n",
        "    is_sch_acc = kwargs.get('is_sch_acc',False)\n",
        "    is_sch_valloss = kwargs.get('is_sch_valloss',False)\n",
        "    is_reg = kwargs.get('is_reg',False)\n",
        "    is_drive = kwargs.get('is_drive',True)\n",
        "    printfreq = kwargs.get('printfreq',100)\n",
        "    reg = 0\n",
        "    if is_early_stopping :\n",
        "        early_stopping = EarlyStopping(patience=40, verbose=True,filename = filename)\n",
        "    if is_sch_epoch:\n",
        "        scheduler_epoch = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[80,160,220,300], gamma=0.1)\n",
        "    if is_sch_valloss:\n",
        "        scheduler_valloss = optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='min',factor = 0.1,patience=15, verbose=True)\n",
        "    if is_sch_acc:\n",
        "        scheduler_acc = optim.lr_scheduler.ReduceLROnPlateau(optimizer,mode='max',factor = 0.1,patience=15, verbose=True)\n",
        "    if is_reg:\n",
        "        reg_type = kwargs.get(\"reg_type\",None)\n",
        "        \n",
        "    for epoch in range(num_epochs):\n",
        "        print('-' * 10)\n",
        "        \n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            losses = AverageMeter('Loss', ':.4e')\n",
        "            top1 = AverageMeter('Acc@1', ':6.2f')\n",
        "            top5 = AverageMeter('Acc@5', ':6.2f')\n",
        "            progress = ProgressMeter(dataset_sizes[phase],[losses, top1, top5],prefix='{}: '.format(phase))\n",
        "            if phase == 'train':\n",
        "                if is_debug: print(\"Train\")\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                if is_debug: print(\"Validation\")\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "            batch_itr = 1\n",
        "            add_param = []\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                \n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs.type(dtype))\n",
        "\n",
        "                    if is_reg and phase == 'train':\n",
        "                        reg = get_reg(model,reg_type,epoch,lr=lr,eta = 1)\n",
        "                        if is_debug: print(\"Reg loss is {}\".format(reg))\n",
        "                    else:\n",
        "                        reg = 0\n",
        "                    loss = criterion(outputs, labels.to(device)) + reg\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                acc1, acc5 = accuracy(outputs, labels.to(device), topk=(1, 5))\n",
        "                losses.update(loss.detach().item(),inputs.size(0))\n",
        "                top1.update(acc1[0], inputs.size(0))\n",
        "                top5.update(acc5[0], inputs.size(0))\n",
        "                if batch_itr % printfreq == 0:\n",
        "                    progress.display(i)\n",
        "            add_param.append(int(epoch))\n",
        "            add_param.append(losses.avg)\n",
        "            add_param.append((top1.avg).tolist())\n",
        "            add_param.append((top5.avg).tolist())\n",
        "            if phase == 'train':\n",
        "                trainframe.loc[epoch] = add_param\n",
        "            if phase == 'val':\n",
        "                testframe.loc[epoch] = add_param  \n",
        "            # deep copy the model\n",
        "            if phase == 'val' and top1.avg > best_acc:\n",
        "                best_acc = top1.avg\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'val' and is_early_stopping:\n",
        "                val_loss = losses.avg\n",
        "                early_stopping(val_loss, model)\n",
        "                if early_stopping.early_stop:\n",
        "                    print(\"Early stopping\")\n",
        "                    break\n",
        "            print(\"{} epoch : {}, accuray: {},Loss : {} \".format(phase,int(epoch),top1.avg,losses.avg))\n",
        "        if is_sch_epoch:\n",
        "            scheduler_epoch.step(epoch)\n",
        "        if is_sch_valloss:\n",
        "            val_loss = losses.avg\n",
        "            scheduler_valloss.step(val_loss)\n",
        "        if is_sch_acc:\n",
        "            acc1 = top1.avg\n",
        "            scheduler_acc.step(acc1)\n",
        "        if is_drive:\n",
        "            trainframe.to_excel(\"drive/My Drive/Model Results/{}_train.xlsx\".format(filename))\n",
        "            testframe.to_excel(\"drive/My Drive/Model Results/{}_test.xlsx\".format(filename)) \n",
        "        else:\n",
        "            trainframe.to_excel(\"{}_train.xlsx\".format(filename))\n",
        "            testframe.to_excel(\"{}_test.xlsx\".format(filename)) \n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}